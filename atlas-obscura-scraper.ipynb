{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 1 Scrape the info card\n",
    "\n",
    "![info cards.png](http://url/to/img.png)\n",
    "\n",
    "The first step of the task is to scrape the info card of each POI, which contains titles, geolocations,\n",
    "urls to the main article page, etc. Those ugly try and except clauses are a necessary evil,\n",
    "because not each info card has all the information, so errors may arise during the scraping!"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "def parse_card(card, data):\n",
    "    try:\n",
    "        data['title'].append(card.select_one('div > a > div > h3 > span').text)\n",
    "    except:\n",
    "        data['title'].append('')\n",
    "\n",
    "    try:\n",
    "        data['country'].append(card.select_one('div > a')['data-country'])\n",
    "    except:\n",
    "        data['country'].append()\n",
    "\n",
    "    try:\n",
    "        data['city'].append(card.select_one('div > a')['data-city'])\n",
    "    except:\n",
    "        data['city'].append('')\n",
    "\n",
    "    try:\n",
    "        data['description_short'].append(card.select_one('div > a > div > div.Card__content.js-subtitle-content').text.strip())\n",
    "    except:\n",
    "        data['description_short'].append('')\n",
    "\n",
    "    try:\n",
    "        data['page_url'].append('www.atlasobscura.com' + card.select_one('div > a')['href'])\n",
    "    except:\n",
    "        data['page_url'].append('')\n",
    "\n",
    "    try:\n",
    "        data['latitude'].append(card.select_one('div > a')['data-lat'])\n",
    "    except:\n",
    "        data['latitude'].append('')\n",
    "\n",
    "    try:\n",
    "        data['longitude'].append(card.select_one('div > a')['data-lng'])\n",
    "    except:\n",
    "        data['longitude'].append('')\n",
    "\n",
    "    try:\n",
    "        data['img_url'].append(card.select_one('div > a > figure > img')['data-src'])\n",
    "    except:\n",
    "        data['img_url'].append('')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 done\n",
      "Page 2 done\n",
      "Page 3 done\n",
      "Page 4 done\n"
     ]
    }
   ],
   "source": [
    "data = {'title': [],\n",
    "        'country': [],\n",
    "        'city': [],\n",
    "        'description_short': [],\n",
    "        'page_url': [],\n",
    "        'latitude': [],\n",
    "        'longitude': [],\n",
    "        'img_url': [],\n",
    "        'page': []}\n",
    "\n",
    "# Go over page 1-144\n",
    "for i in range(1, 145):\n",
    "    url = 'https://www.atlasobscura.com/things-to-do/united-kingdom/places?page={}'.format(i)\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    cards = soup.select('.CardWrapper')\n",
    "    for card in cards:\n",
    "        parse_card(card, data)\n",
    "        data['page'].append(i)\n",
    "    print('Page {} done'.format(i))\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the final result\n",
    "df.to_excel('atlas-obscura-data-iter1.xlsx', index=False)\n",
    "df.to_excel('atlas-obscura-data-latest.xlsx', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Step 2 Scrape each article\n",
    "\n",
    "![article.png](http://url/to/img.png)\n",
    "\n",
    "Now that we have got each POI's url to the main article site, we can finally scrape them!\n",
    "However, as it is gonna be a long journey,\n",
    "it's important to save the intermediary every now and then."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "def scrape_page(url):\n",
    "    url = 'https://' + url\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    result = {}\n",
    "    try:\n",
    "        result['html'] = str(soup)\n",
    "    except:\n",
    "        result['html'] = ''\n",
    "    try:\n",
    "        result['url-site'] = soup.select_one('#place-container > div.DDPage__content-row.grid-row > div.DDPageSiderail__column.grid-col-lg-4.grid-col-md-5 > div.DDPageSiderail > aside.DDPageSiderail__details > div.DDPageSiderail__website.hidden-print > a')['href']\n",
    "    except:\n",
    "        result['url-site'] = ''\n",
    "    try:\n",
    "        result['url-google-map'] = soup.select_one('.DDPageSiderail__directions-link')['href']\n",
    "    except:\n",
    "        result['url-google-map'] = ''\n",
    "    try:\n",
    "        result['article'] = soup.select_one('#place-body').text.strip() + '\\n' + soup.select_one('#place-container > div.DDPage__content-row.grid-row > div.DDPageContent__column.grid-col-lg-6.grid-col-md-7 > div.DDP__direction-copy').text.strip()\n",
    "    except:\n",
    "        result['article'] = soup.select_one('#place-body').text.strip()\n",
    "    try:\n",
    "        result['tags'] = soup.select('.itemTags__link')\n",
    "        result['tags'] = ', '.join([item.text.strip() for item in result['tags']])\n",
    "    except:\n",
    "        result['tags'] = ''\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# You may need to run the result data over the same code again, in case there are missing articles.\n",
    "df = pd.read_csv('atlas-obscura-data-latest.xlsx')\n",
    "cols = ['url-site', 'url-google-map', 'article', 'tags', 'html']\n",
    "for col in cols:\n",
    "    df[col] = ''\n",
    "\n",
    "for i, url in enumerate(df['page_url']):\n",
    "    if df['article'].iloc[i] == '':\n",
    "        result = scrape_page(url)\n",
    "        for col in cols:\n",
    "            df[col].iloc[i] = result[col]\n",
    "        print('POI {} Done'.format(i))\n",
    "        time.sleep(2)\n",
    "    if i % 200 == 0:\n",
    "        df.to_excel('atlas-obscura-data-iter2-{}.xlsx', index=False)\n",
    "        df.to_excel('atlas-obscura-data-latest.xlsx', index=False)\n",
    "\n",
    "# Save the final result\n",
    "df.to_excel('atlas-obscura-data-iter2-{}.xlsx', index=False)\n",
    "df.to_excel('atlas-obscura-data-latest.xlsx', index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}